{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the records\n",
    "folder_path = 'C:\\\\Users\\\\jacob\\\\Desktop\\\\GitHub\\\\CS593-ProjectFiles\\\\Project 3\\\\ECG Anomoly Detection\\\\Dataset'\n",
    "\n",
    "# List of record names\n",
    "records = [f[:-4] for f in os.listdir(folder_path) if f.endswith('.dat')]\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Window size\n",
    "window_size = 3600\n",
    "\n",
    "# Loop over all records\n",
    "for record_name in records:\n",
    "    try:\n",
    "        # Load the record and the annotations\n",
    "        record = wfdb.rdrecord(os.path.join(folder_path, record_name))\n",
    "        annotation = wfdb.rdann(os.path.join(folder_path, record_name), 'atr')\n",
    "\n",
    "        # Pad the signal data with zeros until its length is a multiple of the window size\n",
    "        padded_length = np.ceil(record.p_signal.shape[0] / window_size) * window_size\n",
    "        padded_signal = np.pad(record.p_signal, ((0, int(padded_length - record.p_signal.shape[0])), (0, 0)))\n",
    "\n",
    "        # Reshape the padded signal data into windows\n",
    "        X = np.reshape(padded_signal, (-1, window_size, 2))\n",
    "\n",
    "        # Create labels for each window based on the annotations\n",
    "        Y = np.zeros(X.shape[0])\n",
    "        for i in range(len(annotation.sample)):\n",
    "            if annotation.symbol[i] != 'N':\n",
    "                Y[annotation.sample[i] // window_size] = 1\n",
    "\n",
    "        # Append the data and the labels to the lists\n",
    "        data.append(X)\n",
    "        labels.append(Y)\n",
    "    except:\n",
    "        print(f\"Error loading record {record_name}\")\n",
    "\n",
    "# Concatenate all the data and labels\n",
    "data = np.concatenate(data)\n",
    "labels = np.concatenate(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data, axis=(0, 1))\n",
    "std = np.std(data, axis=(0, 1))\n",
    "\n",
    "# Standardize the data\n",
    "data = (data - mean) / std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "# Further split the training set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection using Forecasting:\n",
    " This approach can be implemented using a variety of methods, depending on what kind of forecasting model you want to use. One common choice is ARIMA (Autoregressive Integrated Moving Average), which can be implemented using the statsmodels library. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Select the first channel of the first record\n",
    "first_channel = data[0, :, 0]\n",
    "\n",
    "# Create and fit the model\n",
    "model = ARIMA(first_channel, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "\n",
    "# Forecast the next data point\n",
    "forecast = model_fit.forecast()[0]\n",
    "\n",
    "# If the difference between the forecast and the actual next data point is greater than some threshold, consider it an anomaly\n",
    "if abs(forecast - first_channel[-1]) > 0.5:\n",
    "    print(\"Anomaly detected\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Use the model to predict the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m      4\u001b[0m y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m      6\u001b[0m \u001b[39m# The model's output is continuous, but we need binary predictions for the metrics.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# We can choose a threshold (e.g., 0.5) and classify all instances with an output above this threshold as anomalies.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\statsmodels\\base\\model.py:261\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, params, exog, *args, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, params, exog\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    256\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m    After a model has been fit predict returns the fitted values.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[0;32m    259\u001b[0m \u001b[39m    This is a placeholder intended to be overwritten by individual models.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the model to predict the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = y_pred.max(axis=1).flatten()\n",
    "\n",
    "# The model's output is continuous, but we need binary predictions for the metrics.\n",
    "# We can choose a threshold (e.g., 0.5) and classify all instances with an output above this threshold as anomalies.\n",
    "y_pred_bin = (y_pred > 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
