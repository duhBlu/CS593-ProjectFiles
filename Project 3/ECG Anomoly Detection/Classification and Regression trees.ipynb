{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the records\n",
    "folder_path = 'C:\\\\Users\\\\jacob\\\\Desktop\\\\GitHub\\\\CS593-ProjectFiles\\\\Project 3\\\\ECG Anomoly Detection\\\\Dataset'\n",
    "\n",
    "# List of record names\n",
    "records = [f[:-4] for f in os.listdir(folder_path) if f.endswith('.dat')]\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Window size\n",
    "window_size = 3600\n",
    "\n",
    "# Loop over all records\n",
    "for record_name in records:\n",
    "    try:\n",
    "        # Load the record and the annotations\n",
    "        record = wfdb.rdrecord(os.path.join(folder_path, record_name))\n",
    "        annotation = wfdb.rdann(os.path.join(folder_path, record_name), 'atr')\n",
    "\n",
    "        # Pad the signal data with zeros until its length is a multiple of the window size\n",
    "        padded_length = np.ceil(record.p_signal.shape[0] / window_size) * window_size\n",
    "        padded_signal = np.pad(record.p_signal, ((0, int(padded_length - record.p_signal.shape[0])), (0, 0)))\n",
    "\n",
    "        # Reshape the padded signal data into windows\n",
    "        X = np.reshape(padded_signal, (-1, window_size, 2))\n",
    "\n",
    "        # Create labels for each window based on the annotations\n",
    "        Y = np.zeros(X.shape[0])\n",
    "        for i in range(len(annotation.sample)):\n",
    "            if annotation.symbol[i] != 'N':\n",
    "                Y[annotation.sample[i] // window_size] = 1\n",
    "\n",
    "        # Append the data and the labels to the lists\n",
    "        data.append(X)\n",
    "        labels.append(Y)\n",
    "    except:\n",
    "        print(f\"Error loading record {record_name}\")\n",
    "\n",
    "# Concatenate all the data and labels\n",
    "data = np.concatenate(data)\n",
    "labels = np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data, axis=(0, 1))\n",
    "std = np.std(data, axis=(0, 1))\n",
    "\n",
    "# Standardize the data\n",
    "data = (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "# Further split the training set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees (CART): \n",
    "You can use the sklearn library to create decision trees or isolation forests. Here's a basic example of how to create an isolation forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[39m# Create and fit the model\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m IsolationForest(contamination\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n",
      "\u001b[1;32m----> 5\u001b[0m model\u001b[39m.\u001b[39mfit(X_train\u001b[39m.\u001b[39mreshape(X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;32m      7\u001b[0m \u001b[39m# Predict the anomalies in the data\u001b[39;00m\n",
      "\u001b[0;32m      8\u001b[0m anomalies \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test\u001b[39m.\u001b[39mreshape(X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create and fit the model\n",
    "model = IsolationForest(contamination=0.01)\n",
    "model.fit(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "# Predict the anomalies in the data\n",
    "anomalies = model.predict(X_test.reshape(X_test.shape[0], -1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
