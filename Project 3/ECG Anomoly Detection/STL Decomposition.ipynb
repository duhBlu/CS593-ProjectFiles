{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the records\n",
    "folder_path = 'C:\\\\Users\\\\jacob\\\\Desktop\\\\GitHub\\\\CS593-ProjectFiles\\\\Project 3\\\\ECG Anomoly Detection\\\\Dataset'\n",
    "\n",
    "# List of record names\n",
    "records = [f[:-4] for f in os.listdir(folder_path) if f.endswith('.dat')]\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Window size\n",
    "window_size = 3600\n",
    "\n",
    "# Loop over all records\n",
    "for record_name in records:\n",
    "    try:\n",
    "        # Load the record and the annotations\n",
    "        record = wfdb.rdrecord(os.path.join(folder_path, record_name))\n",
    "        annotation = wfdb.rdann(os.path.join(folder_path, record_name), 'atr')\n",
    "\n",
    "        # Pad the signal data with zeros until its length is a multiple of the window size\n",
    "        padded_length = np.ceil(record.p_signal.shape[0] / window_size) * window_size\n",
    "        padded_signal = np.pad(record.p_signal, ((0, int(padded_length - record.p_signal.shape[0])), (0, 0)))\n",
    "\n",
    "        # Reshape the padded signal data into windows\n",
    "        X = np.reshape(padded_signal, (-1, window_size, 2))\n",
    "\n",
    "        # Create labels for each window based on the annotations\n",
    "        Y = np.zeros(X.shape[0])\n",
    "        for i in range(len(annotation.sample)):\n",
    "            if annotation.symbol[i] != 'N':\n",
    "                Y[annotation.sample[i] // window_size] = 1\n",
    "\n",
    "        # Append the data and the labels to the lists\n",
    "        data.append(X)\n",
    "        labels.append(Y)\n",
    "    except:\n",
    "        print(f\"Error loading record {record_name}\")\n",
    "\n",
    "# Concatenate all the data and labels\n",
    "data = np.concatenate(data)\n",
    "labels = np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data, axis=(0, 1))\n",
    "std = np.std(data, axis=(0, 1))\n",
    "\n",
    "# Standardize the data\n",
    "data = (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "# Further split the training set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up The Model\n",
    "# STL Decomposition: \n",
    "This technique can be implemented using the statsmodels library in Python. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Select the first channel of the first record\n",
    "first_channel = data[0, :, 0]\n",
    "\n",
    "# Decompose the time series\n",
    "decomposition = sm.tsa.seasonal_decompose(first_channel, model='additive', period=window_size)\n",
    "\n",
    "# Plot the original data, the trend, the seasonality, and the residuals \n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
