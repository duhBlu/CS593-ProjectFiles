{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the records\n",
    "folder_path = 'C:\\\\Users\\\\jacob\\\\Desktop\\\\GitHub\\\\CS593-ProjectFiles\\\\Project 3\\\\ECG Anomoly Detection\\\\Dataset'\n",
    "\n",
    "# List of record names\n",
    "records = [f[:-4] for f in os.listdir(folder_path) if f.endswith('.dat')]\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Window size\n",
    "window_size = 3600\n",
    "\n",
    "# Loop over all records\n",
    "for record_name in records:\n",
    "    try:\n",
    "        # Load the record and the annotations\n",
    "        record = wfdb.rdrecord(os.path.join(folder_path, record_name))\n",
    "        annotation = wfdb.rdann(os.path.join(folder_path, record_name), 'atr')\n",
    "\n",
    "        # Pad the signal data with zeros until its length is a multiple of the window size\n",
    "        padded_length = np.ceil(record.p_signal.shape[0] / window_size) * window_size\n",
    "        padded_signal = np.pad(record.p_signal, ((0, int(padded_length - record.p_signal.shape[0])), (0, 0)))\n",
    "\n",
    "        # Reshape the padded signal data into windows\n",
    "        X = np.reshape(padded_signal, (-1, window_size, 2))\n",
    "\n",
    "        # Create labels for each window based on the annotations\n",
    "        Y = np.zeros(X.shape[0])\n",
    "        for i in range(len(annotation.sample)):\n",
    "            if annotation.symbol[i] != 'N':\n",
    "                Y[annotation.sample[i] // window_size] = 1\n",
    "\n",
    "        # Append the data and the labels to the lists\n",
    "        data.append(X)\n",
    "        labels.append(Y)\n",
    "    except:\n",
    "        print(f\"Error loading record {record_name}\")\n",
    "\n",
    "# Concatenate all the data and labels\n",
    "data = np.concatenate(data)\n",
    "labels = np.concatenate(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data, axis=(0, 1))\n",
    "std = np.std(data, axis=(0, 1))\n",
    "\n",
    "# Standardize the data\n",
    "data = (data - mean) / std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "# Further split the training set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection using Forecasting:\n",
    " This approach can be implemented using a variety of methods, depending on what kind of forecasting model you want to use. One common choice is ARIMA (Autoregressive Integrated Moving Average), which can be implemented using the statsmodels library. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtsa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marima\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m ARIMA\n",
      "\u001b[0;32m      3\u001b[0m \u001b[39m# Select the first channel of the first record\u001b[39;00m\n",
      "\u001b[1;32m----> 4\u001b[0m first_channel \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;32m      6\u001b[0m \u001b[39m# Create and fit the model\u001b[39;00m\n",
      "\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m ARIMA(first_channel, order\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m))\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Select the first channel of the first record\n",
    "first_channel = data[0, :, 0]\n",
    "\n",
    "# Create and fit the model\n",
    "model = ARIMA(first_channel, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "\n",
    "# Forecast the next data point\n",
    "forecast = model_fit.forecast()[0]\n",
    "\n",
    "# If the difference between the forecast and the actual next data point is greater than some threshold, consider it an anomaly\n",
    "if abs(forecast - first_channel[-1]) > 0.5:\n",
    "    print(\"Anomaly detected\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the model to predict the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = y_pred.max(axis=1).flatten()\n",
    "\n",
    "# The model's output is continuous, but we need binary predictions for the metrics.\n",
    "# We can choose a threshold (e.g., 0.5) and classify all instances with an output above this threshold as anomalies.\n",
    "y_pred_bin = (y_pred > 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
